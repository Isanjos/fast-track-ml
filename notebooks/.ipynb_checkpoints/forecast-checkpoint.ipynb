{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20ce3cae-4f65-48c2-b455-9bd8ae810833",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Previsão de Colaboradores aaaaa\n",
    "## Sumário de Conteúdos:\n",
    "---\n",
    "#### [**[AI Forecast]** Configurações Iniciais](#prerequisites)\n",
    "   - [**1.1** Import de Bibliotecas](#prerequisites_authentication)\n",
    "   - [**1.2** Definição de Funções](#prerequisites_policies)\n",
    "   - [**1.3** Definição de Variáveis](#prerequisites_policies)\n",
    "\n",
    "#### [**[23AI]** Conexão com Banco de Dados](#prerequisites)\n",
    "#### [**[Pre-Model]** Limpeza e Tratamento de Dados](#prerequisites)\n",
    "   - [**3.1** Divisão do Dataset - Treino | Teste](#prerequisites_authentication)\n",
    "   - [**3.2** Armazenamento dos Datasets - Treino | Teste](#prerequisites_authentication)\n",
    "   - [**3.3** Criação dos arquivos YAML - Forecast AI](#prerequisites_authentication)\n",
    "#### [**[FORECAST]** União das tabelas 1510 | 5428 | 7072](#prerequisites)\n",
    "#### [**[FORECAST]** Encaminhamento das tabelas ao Banco](#prerequisites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ac8f0a-ddc1-47b6-a112-64980d567c2b",
   "metadata": {},
   "source": [
    "# **[AI Forecast]** Configurações Iniciais"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c5a9f99-934e-4159-9484-6d53b6195046",
   "metadata": {
    "tags": []
   },
   "source": [
    "ads operator init -t forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d64266-6506-469d-aad2-ed40e9b3f5cf",
   "metadata": {},
   "source": [
    "## **1.1** Import de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d954a6d1-2240-4d77-b4b8-72b91545a667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import oci\n",
    "import ads\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import oracledb\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "ads.set_auth(auth='resource_principal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a987dc-255c-4ae8-9cc5-f9f7ae979dc0",
   "metadata": {},
   "source": [
    "## **1.2** Definição de Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e228c9c-4cdb-4465-aed3-0da3646c476a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_table_to_dataframe(conn, table_name):\n",
    "    \"\"\"\n",
    "    Carrega os dados de uma tabela do banco de dados em um DataFrame.\n",
    "\n",
    "    Parâmetros:\n",
    "        conn: Conexão ativa com o banco de dados.\n",
    "        table_name: Nome da tabela a ser carregada.\n",
    "\n",
    "    Retorno:\n",
    "        DataFrame contendo os dados da tabela.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    return pd.read_sql(query, conn)\n",
    "\n",
    "def remove_duplicates(data, timestamp_col):\n",
    "    \"\"\"\n",
    "    Remove duplicatas de um DataFrame\n",
    "\n",
    "    Parâmetros:\n",
    "        data (pd.DataFrame): Dataset a ser processado.\n",
    "        timestamp_col (str): Nome da coluna de timestamp.\n",
    "\n",
    "    Retorna:\n",
    "        pd.DataFrame: Dataset sem duplicatas.\n",
    "    \"\"\"\n",
    "    if data.duplicated(subset=[timestamp_col]).any():\n",
    "        print(f\"Valores duplicados encontrados na coluna '{timestamp_col}'. Corrigindo...\")\n",
    "        data = data.drop_duplicates(subset=[timestamp_col], keep='first')\n",
    "\n",
    "    if data.duplicated(subset=[timestamp_col]).any():\n",
    "        raise ValueError(f\"Ainda há valores duplicados na coluna '{timestamp_col}'. Verifique o dataset.\")\n",
    "\n",
    "    print(f\"Coluna '{timestamp_col}' validada e sem duplicatas.\")\n",
    "    return data\n",
    "\n",
    "def remove_nulls(data, timestamp_col):\n",
    "    \"\"\"\n",
    "    Remove linhas com valores nulos.\n",
    "\n",
    "    Parâmetros:\n",
    "        data (pd.DataFrame): Dataset a ser processado.\n",
    "        timestamp_col (str): Nome da coluna a ser verificada.\n",
    "\n",
    "    Retorna:\n",
    "        pd.DataFrame: Dataset sem valores nulos na coluna.\n",
    "    \"\"\"\n",
    "    null_count = data[timestamp_col].isnull().sum()\n",
    "    if null_count > 0:\n",
    "        print(f\"Valores nulos encontrados na coluna '{timestamp_col}': {null_count}. Removendo...\")\n",
    "        data = data.dropna(subset=[timestamp_col])\n",
    "    \n",
    "    null_count_after = data[timestamp_col].isnull().sum()\n",
    "    if null_count_after == 0:\n",
    "        print(f\"Todos os valores nulos foram removidos da coluna '{timestamp_col}'.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Ainda há valores nulos na coluna '{timestamp_col}' após a tentativa de remoção.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def filter_by_cod_local(df, cod_local_list):\n",
    "    return {cod: df[df['COD_LOCAL'] == cod] for cod in cod_local_list}\n",
    "\n",
    "def split_dataset(data, timestamp_col, target_col, test_size):\n",
    "    \"\"\"\n",
    "    Divide um dataset temporal em treino, teste e dados adicionais, preservando a ordem temporal.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Dataset completo.\n",
    "        timestamp_col (str): Nome da coluna que representa o timestamp.\n",
    "        target_col (str): Nome da coluna alvo (target) para a previsão.\n",
    "        series_col (str): Nome da coluna que identifica a série temporal.\n",
    "        test_size (float): Proporção do conjunto de teste (default=0.2).\n",
    "\n",
    "    Returns:\n",
    "        train_data (pd.DataFrame): Conjunto de treino.\n",
    "        test_data (pd.DataFrame): Conjunto de teste.\n",
    "        additional_data (pd.DataFrame): Conjunto adicional, se necessário.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ordena os dados pelo timestamp para garantir a preservação da sequência temporal.\n",
    "    data = data.sort_values(by=timestamp_col)\n",
    "    \n",
    "    # Converte a coluna de timestamp para o tipo datetime, garantindo consistência nos dados temporais.\n",
    "    data[timestamp_col] = pd.to_datetime(data[timestamp_col], format='%d-%b-%y %H:%M:%S', errors='coerce')\n",
    "\n",
    "    # Seleciona apenas as colunas necessárias (timestamp e target) para evitar excesso de dados irrelevantes.\n",
    "    colunas= [timestamp_col, target_col]\n",
    "    data = data[colunas]\n",
    "    \n",
    "    # Calcula o índice de separação entre os dados de treino e teste com base no tamanho do conjunto de teste.\n",
    "    split_index = int(len(data) * (1 - test_size))\n",
    "    \n",
    "    # Divide o dataset em treino e teste com base no índice calculado.\n",
    "    train_data = data.iloc[:split_index]  # Dados de treino.\n",
    "    test_data = data.iloc[split_index:]  # Dados de teste.\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def save_datasets_by_type(results_turno, results_hora, base_folder_turno, base_folder_hora):\n",
    "    \"\"\"\n",
    "    Salva os datasets de treino, teste e adicional (quando disponível) em pastas separadas por tipo e código.\n",
    "\n",
    "    Parâmetros:\n",
    "        results_turno (dict): Dicionário contendo os datasets de turno processados.\n",
    "        results_hora (dict): Dicionário contendo os datasets de hora processados.\n",
    "        base_folder_turno (str): Caminho base para salvar os arquivos de turno.\n",
    "        base_folder_hora (str): Caminho base para salvar os arquivos de hora.\n",
    "    \"\"\"\n",
    "    # Função interna para salvar datasets\n",
    "    def save_datasets(results, base_folder, include_additional):\n",
    "        for name, data in results.items():\n",
    "            print(f\"Salvando dados para {name}...\")\n",
    "\n",
    "            # Extraindo o código do nome do dataset\n",
    "            code = name.split('_')[1]\n",
    "            \n",
    "            # Criando a pasta específica para o código\n",
    "            code_folder = os.path.join(base_folder, f\"COPEL_{code}\")\n",
    "            os.makedirs(code_folder, exist_ok=True)\n",
    "            \n",
    "            # Criando caminhos para os arquivos CSV\n",
    "            train_data_path = os.path.join(code_folder, \"COPEL_TRAIN.csv\")\n",
    "            test_data_path = os.path.join(code_folder, \"COPEL_TEST.csv\")\n",
    "\n",
    "            # Salvando os datasets de treino e teste\n",
    "            data[\"train_data\"].to_csv(train_data_path, index=False)\n",
    "            data[\"test_data\"].to_csv(test_data_path, index=False)\n",
    "\n",
    "            # Salvando o conjunto adicional se estiver disponível\n",
    "            if include_additional and \"additional_data\" in data:\n",
    "                additional_data_path = os.path.join(code_folder, \"COPEL_DATA_ADDITIONAL.csv\")\n",
    "                data[\"additional_data\"].to_csv(additional_data_path, index=False)\n",
    "                print(f\"Arquivos salvos em {code_folder}:\" +\n",
    "                      f\"\\n - {train_data_path}\" +\n",
    "                      f\"\\n - {test_data_path}\" +\n",
    "                      f\"\\n - {additional_data_path}\")\n",
    "            else:\n",
    "                print(f\"Arquivos salvos em {code_folder}:\" +\n",
    "                      f\"\\n - {train_data_path}\" +\n",
    "                      f\"\\n - {test_data_path}\")\n",
    "\n",
    "    # Salvando datasets de turno\n",
    "    print(\"\\nSalvando datasets de turno...\")\n",
    "    save_datasets(results_turno, base_folder_turno, include_additional=True)\n",
    "\n",
    "    # Salvando datasets de hora\n",
    "    print(\"\\nSalvando datasets de hora...\")\n",
    "    save_datasets(results_hora, base_folder_hora, include_additional=False)\n",
    "\n",
    "def create_forecast_yaml(datasets, base_folder, output_folder, \n",
    "                         timestamp_col, target_col, horizon, model):\n",
    "    \"\"\"\n",
    "    Cria arquivos YAML para cada dataset, com configuração de previsão e gera comandos para execução.\n",
    "\n",
    "    Parâmetros:\n",
    "        datasets (dict): Dicionário contendo os datasets processados.\n",
    "        base_folder (str): Caminho base para os arquivos CSV.\n",
    "        output_folder (str): Pasta onde os YAMLs serão salvos.\n",
    "        timestamp_col (str): Nome da coluna de timestamp.\n",
    "        target_col (str): Nome da coluna alvo (target).\n",
    "        horizon (int): Horizonte de previsão.\n",
    "        model (str): Modelo de previsão.\n",
    "\n",
    "    Retorna:\n",
    "        commands (list): Lista de strings com comandos `ads operator run`.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    commands = []\n",
    "    \n",
    "    for name in datasets.keys():\n",
    "        # Extrai o código do dataset\n",
    "        code = name.split('_')[1]\n",
    "        \n",
    "        # Caminhos dos datasets\n",
    "        train_data_path = os.path.join(base_folder, f\"COPEL_{code}\", \"COPEL_TRAIN.csv\")\n",
    "        test_data_path = os.path.join(base_folder, f\"COPEL_{code}\", \"COPEL_TEST.csv\")\n",
    "        \n",
    "        # Configuração do YAML\n",
    "        forecast_config = {\n",
    "            \"kind\": \"operator\",\n",
    "            \"spec\": {\n",
    "                \"datetime_column\": {\n",
    "                    \"name\": timestamp_col\n",
    "                },\n",
    "                \"historical_data\": {\n",
    "                    \"url\": train_data_path\n",
    "                },\n",
    "                \"test_data\": {\n",
    "                    \"url\": test_data_path\n",
    "                },\n",
    "                \"output_directory\": {\n",
    "                    \"url\": f\"COPEL_OUTPUT/{code}/{target_col}\"\n",
    "                },\n",
    "                \"model\": model,\n",
    "                \"target_column\": target_col,\n",
    "                \"horizon\": horizon\n",
    "            },\n",
    "            \"type\": \"forecast\",\n",
    "            \"version\": \"v1\"\n",
    "        }\n",
    "        \n",
    "        # Nome do YAML\n",
    "        yaml_file_path = os.path.join(output_folder, f\"FORECAST_{code}_{target_col}.yaml\")\n",
    "        \n",
    "        # Salva o YAML no arquivo\n",
    "        with open(yaml_file_path, \"w\") as file:\n",
    "            yaml.dump(forecast_config, file)\n",
    "        \n",
    "        print(f\"Arquivo YAML criado: {yaml_file_path}\")\n",
    "        \n",
    "        # Adiciona o comando à lista\n",
    "        command = f\"ads operator run -f {yaml_file_path} -b local\"\n",
    "        commands.append(command)\n",
    "    \n",
    "    return commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989c2db-7f3c-4c1e-ac40-c0debfe0fe07",
   "metadata": {},
   "source": [
    "## **1.3** Definição de Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "516db7df-aa24-4f5d-bdfb-fcdbc5a711cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timestamp_col = \"HORARIO_COMPLETO\"\n",
    "\n",
    "target_col_turno='INDIVIDUO_TURNO'\n",
    "target_col_hora='TEMPO_TOTAL_SIMPLES'\n",
    "\n",
    "base_folder_turno=f\"COPEL_INPUT/{target_col_turno}\"\n",
    "base_folder_hora=f\"COPEL_INPUT/{target_col_hora}\"\n",
    "\n",
    "output_folder_turno= f\"forecast/COPEL/TURNO/{target_col_turno}\"\n",
    "output_folder_hora= f\"forecast/COPEL/HORA/{target_col_hora}\"\n",
    "\n",
    "model=\"prophet\"\n",
    "       # - prophet\n",
    "       #  - arima\n",
    "       #  - neuralprophet\n",
    "       #  - lgbforecast\n",
    "       #  - automlx\n",
    "       #  - autots\n",
    "       #  - auto-select\n",
    "\n",
    "horizon_hora=600\n",
    "horizon_turno=100\n",
    "\n",
    "test_size_hora=0.1\n",
    "test_size_turno=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfbe57f-3aa8-47da-a39a-a21217c9841d",
   "metadata": {},
   "source": [
    "## **[23AI]** Conexão com Banco de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba8ae8b-dadf-4cd0-b599-25fa918b7219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn = oracledb.connect(user=\"FORECAST\", password=\"Oracle123#analytics\", dsn=\"(description= (retry_count=20)(retry_delay=3)(address=(protocol=tcps)(port=1522)(host=adb.us-ashburn-1.oraclecloud.com))(connect_data=(service_name=arj21cwbhl1zqnf_opendata_medium.adwc.oraclecloud.com))(security=(ssl_server_dn_match=yes)))\",\n",
    "                              config_dir=\"/home/datascience/[WALLET] 23AI\",\n",
    "                              wallet_location=\"/home/datascience/[WALLET] 23AI\",\n",
    "                              wallet_password=\"Oracle123#analytics\")\n",
    "    print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(\"Connection failed!\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c4c36b4-e597-4a27-acc7-69bb46b005d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Busca os dados do banco\n",
    "with conn: \n",
    "    df_turno = load_table_to_dataframe(conn, \"FORECAST.EQUIPE_TURNO_INDIV\")\n",
    "    df_hora = load_table_to_dataframe(conn, \"FORECAST.EQUIPE_HORA_INDIV\")\n",
    "\n",
    "# Filtra por código-local\n",
    "df_hora[\"COD_LOCAL\"] = df_hora[\"COD_LOCAL\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dda66f-833e-40fd-9720-b72483aac0cd",
   "metadata": {},
   "source": [
    "## **[Pre-Model]** Limpeza e Tratamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b899f0-beb3-4259-bb2d-ae0477a1167a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filtered_turno_data':         ID COD_LOCAL        DIA    HORARIO_COMPLETO      TURNO  EQUIPE  \\\n",
       " 0    10207      5428  22-DEC-08 2022-12-08 09:00:00     Diurno     427   \n",
       " 1    19864      7072  23-FEB-11 2023-02-11 19:00:00    Noturno       1   \n",
       " 2     9342      7072  22-NOV-21 2022-11-21 09:00:00     Diurno     336   \n",
       " 3     9421      1510  23-JAN-27 2023-01-27 09:00:00     Diurno     288   \n",
       " 4     9440      7072  23-FEB-16 2023-02-16 19:00:00    Noturno      72   \n",
       " ..     ...       ...        ...                 ...        ...     ...   \n",
       " 857  66796      7072  22-DEC-20 2022-12-20 01:00:00  Madrugada      12   \n",
       " 858  66878      7072  23-JAN-12 2023-01-12 19:00:00    Noturno     118   \n",
       " 859  66885      7072  23-JAN-16 2023-01-16 01:00:00  Madrugada       5   \n",
       " 860  69646      1510  23-JAN-29 2023-01-29 01:00:00  Madrugada       2   \n",
       " 861  69764      7072  22-NOV-14 2022-11-14 19:00:00    Noturno      67   \n",
       " \n",
       "      TEMPO_TOTAL_SIMPLES   TEMPO_TOTAL  ATIVIDADE  INDIVIDUO_TURNO  \\\n",
       " 0           20975.116667  38318.666667        382        79.830556   \n",
       " 1              60.550000     60.550000          1         0.126146   \n",
       " 2            5261.583333   7807.383333        254        16.265382   \n",
       " 3            6912.183333  10081.800000        224        21.003750   \n",
       " 4            1598.766667   1858.766667         62         3.872431   \n",
       " ..                   ...           ...        ...              ...   \n",
       " 857           326.316667    563.633333          7         1.174236   \n",
       " 858          1839.200000   2818.283333         91         5.871424   \n",
       " 859           126.000000    220.000000          3         0.458333   \n",
       " 860            63.000000    126.000000          1         0.262500   \n",
       " 861          1119.650000   1528.566667         52         3.184514   \n",
       " \n",
       "      PESSOA_TURNO  \n",
       " 0        0.668604  \n",
       " 1        0.990917  \n",
       " 2        2.582171  \n",
       " 3        1.713980  \n",
       " 4        2.324122  \n",
       " ..            ...  \n",
       " 857      1.277426  \n",
       " 858      2.512168  \n",
       " 859      1.363636  \n",
       " 860      0.952381  \n",
       " 861      2.629915  \n",
       " \n",
       " [862 rows x 11 columns],\n",
       " 'filtered_hora_data':           ID COD_LOCAL        DIA HORA    HORARIO_COMPLETO      TURNO  EQUIPE  \\\n",
       " 0      21788      7072 2001-12-22   06 2022-12-01 06:00:00  Madrugada       2   \n",
       " 1      21717      5428 2005-12-22   20 2022-12-05 20:00:00    Noturno       2   \n",
       " 2        529      7072 2001-11-22   11 2022-11-01 11:00:00     Diurno      46   \n",
       " 3        522      5428 2001-11-22   12 2022-11-01 12:00:00     Diurno      13   \n",
       " 4       2831      1510 2001-11-22   21 2022-11-01 21:00:00    Noturno       2   \n",
       " ...      ...       ...        ...  ...                 ...        ...     ...   \n",
       " 4016  165287      5428 2016-11-22   10 2022-11-16 10:00:00     Diurno      99   \n",
       " 4017  166105      7072 2016-11-22   12 2022-11-16 12:00:00     Diurno      37   \n",
       " 4018  180940      5428 2010-02-23   05 2023-02-10 05:00:00  Madrugada       1   \n",
       " 4019  184193      7072 2029-01-23   14 2023-01-29 14:00:00     Diurno       3   \n",
       " 4020  183987      7072 2002-02-23   20 2023-02-02 20:00:00    Noturno       2   \n",
       " \n",
       "       TEMPO_TOTAL_SIMPLES  TEMPO_TOTAL  INDIVIDUO_HORA  ATIVIDADE  PESSOA_HORA  \n",
       " 0              103.000000   206.000000        3.433333          1     1.165049  \n",
       " 1               33.000000    66.000000        1.100000          1     3.636364  \n",
       " 2              840.366667   995.466667       16.591111         43     3.284281  \n",
       " 3              272.000000   300.000000        5.000000         12     2.867647  \n",
       " 4               36.000000    72.000000        1.200000          1     3.333333  \n",
       " ...                   ...          ...             ...        ...          ...  \n",
       " 4016           680.000000   843.000000       14.050000         92     8.735294  \n",
       " 4017           489.650000   625.616667       10.426944         33     4.533851  \n",
       " 4018            31.000000    31.000000        0.516667          1     1.935484  \n",
       " 4019           138.383333   138.383333        2.306389          3     1.300735  \n",
       " 4020            42.000000    84.000000        1.400000          1     2.857143  \n",
       " \n",
       " [4021 rows x 12 columns]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88803f65-8e99-49b3-9c31-631de8516a28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores duplicados encontrados na coluna 'HORARIO_COMPLETO'. Corrigindo...\n",
      "Coluna 'HORARIO_COMPLETO' validada e sem duplicatas.\n",
      "Todos os valores nulos foram removidos da coluna 'HORARIO_COMPLETO'.\n",
      "Valores duplicados encontrados na coluna 'HORARIO_COMPLETO'. Corrigindo...\n",
      "Coluna 'HORARIO_COMPLETO' validada e sem duplicatas.\n",
      "Todos os valores nulos foram removidos da coluna 'HORARIO_COMPLETO'.\n"
     ]
    }
   ],
   "source": [
    "# Dicionário contendo os datasets que precisam de processamento\n",
    "datasets_to_process = {\n",
    "    \"filtered_turno_data\": df_turno,\n",
    "    \"filtered_hora_data\": df_hora\n",
    "}\n",
    "\n",
    "processed_turno_data = {}\n",
    "processed_hora_data = {}\n",
    "\n",
    "for dataset_name, dataset in datasets_to_process.items():\n",
    "        dataset = remove_duplicates(dataset, timestamp_col=timestamp_col)\n",
    "        dataset = remove_nulls(dataset, timestamp_col=timestamp_col)\n",
    "\n",
    "        # Armazena no dicionário correto\n",
    "        if dataset_name == \"filtered_turno_data\":\n",
    "            processed_turno_data = dataset\n",
    "        elif dataset_name == \"filtered_hora_data\":\n",
    "            processed_hora_data = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0668916-8046-4c14-8cb7-c444081468b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando turno_1510...\n",
      "Processando turno_5428...\n",
      "Processando turno_7072...\n"
     ]
    }
   ],
   "source": [
    "datasets_turno = { \n",
    "    f\"turno_{cod}\": processed_turno_data[cod] for cod in cod_locais\n",
    "} \n",
    "\n",
    "results_turno = {}\n",
    "\n",
    "# Processar os datasets de turno\n",
    "for name, dataset in datasets_turno.items():\n",
    "    print(f\"Processando {name}...\")\n",
    "    \n",
    "    # Dividir os dados usando a função split_dataset\n",
    "    train_data, test_data = split_dataset(\n",
    "        data=dataset,\n",
    "        timestamp_col=timestamp_col,  \n",
    "        target_col=target_col_turno,     \n",
    "        test_size=test_size_turno\n",
    "    )\n",
    "    \n",
    "    # Armazenar os resultados para cada dataset de turno\n",
    "    results_turno[name] = {\n",
    "        \"train_data\": train_data,\n",
    "        \"test_data\": test_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf96de-12be-4c6c-b929-bf92f3be6c35",
   "metadata": {},
   "source": [
    "## **3.1** Divisão do Dataset - Treino | Teste "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c08c82b1-a9a7-4746-ab5f-b1c6089e2730",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando hora...\n"
     ]
    }
   ],
   "source": [
    "datasets_hora = { \n",
    "    f\"hora\": processed_hora_data\n",
    "}\n",
    "\n",
    "results_hora = {}\n",
    "\n",
    "# Processar os datasets de turno\n",
    "for name, dataset in datasets_hora.items():\n",
    "    print(f\"Processando {name}...\")\n",
    "    \n",
    "    # Dividir os dados usando a função split_dataset\n",
    "    train_data, test_data = split_dataset(\n",
    "        data=dataset,\n",
    "        timestamp_col=timestamp_col,  \n",
    "        target_col=target_col_hora,     \n",
    "        test_size=test_size_hora\n",
    "    )\n",
    "    \n",
    "    # Armazenar os resultados para cada dataset de turno\n",
    "    results_hora[name] = {\n",
    "        \"train_data\": train_data,\n",
    "        \"test_data\": test_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd30c81-e2e8-431f-b8a7-59cf0121afa8",
   "metadata": {},
   "source": [
    "## **3.2** Armazenamento dos Datasets - Treino | Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6331632-9a93-48c0-aee7-8c231905cae2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Salvando datasets de turno...\n",
      "Salvando dados para turno_1510...\n",
      "Arquivos salvos em COPEL_INPUT/INDIVIDUO_TURNO/COPEL_1510:\n",
      " - COPEL_INPUT/INDIVIDUO_TURNO/COPEL_1510/COPEL_TRAIN.csv\n",
      " - COPEL_INPUT/INDIVIDUO_TURNO/COPEL_1510/COPEL_TEST.csv\n",
      "Salvando dados para turno_5428...\n",
      "Arquivos salvos em COPEL_INPUT/INDIVIDUO_TURNO/COPEL_5428:\n",
      " - COPEL_INPUT/INDIVIDUO_TURNO/COPEL_5428/COPEL_TRAIN.csv\n",
      " - COPEL_INPUT/INDIVIDUO_TURNO/COPEL_5428/COPEL_TEST.csv\n",
      "Salvando dados para turno_7072...\n",
      "Arquivos salvos em COPEL_INPUT/INDIVIDUO_TURNO/COPEL_7072:\n",
      " - COPEL_INPUT/INDIVIDUO_TURNO/COPEL_7072/COPEL_TRAIN.csv\n",
      " - COPEL_INPUT/INDIVIDUO_TURNO/COPEL_7072/COPEL_TEST.csv\n",
      "\n",
      "Salvando datasets de hora...\n",
      "Salvando dados para hora_1510...\n",
      "Arquivos salvos em COPEL_INPUT/TEMPO_TOTAL_SIMPLES/COPEL_1510:\n",
      " - COPEL_INPUT/TEMPO_TOTAL_SIMPLES/COPEL_1510/COPEL_TRAIN.csv\n",
      " - COPEL_INPUT/TEMPO_TOTAL_SIMPLES/COPEL_1510/COPEL_TEST.csv\n",
      "Salvando dados para hora_5428...\n",
      "Arquivos salvos em COPEL_INPUT/TEMPO_TOTAL_SIMPLES/COPEL_5428:\n",
      " - COPEL_INPUT/TEMPO_TOTAL_SIMPLES/COPEL_5428/COPEL_TRAIN.csv\n",
      " - COPEL_INPUT/TEMPO_TOTAL_SIMPLES/COPEL_5428/COPEL_TEST.csv\n",
      "Salvando dados para hora_7072...\n",
      "Arquivos salvos em COPEL_INPUT/TEMPO_TOTAL_SIMPLES/COPEL_7072:\n",
      " - COPEL_INPUT/TEMPO_TOTAL_SIMPLES/COPEL_7072/COPEL_TRAIN.csv\n",
      " - COPEL_INPUT/TEMPO_TOTAL_SIMPLES/COPEL_7072/COPEL_TEST.csv\n"
     ]
    }
   ],
   "source": [
    "save_datasets_by_type(\n",
    "    results_turno=results_turno,\n",
    "    results_hora=results_hora,\n",
    "    base_folder_turno=base_folder_turno,\n",
    "    base_folder_hora='CATEGORY_HORA/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ecb42-6f3b-4fb6-86a2-6e4ebcd03b94",
   "metadata": {},
   "source": [
    "## **3.3** Criação dos arquivos YAML - Forecast AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5218702f-7013-4033-b8cc-0b414e5ea14b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo YAML criado: forecast/COPEL/TURNO/INDIVIDUO_TURNO/FORECAST_1510_INDIVIDUO_TURNO.yaml\n",
      "Arquivo YAML criado: forecast/COPEL/TURNO/INDIVIDUO_TURNO/FORECAST_5428_INDIVIDUO_TURNO.yaml\n",
      "Arquivo YAML criado: forecast/COPEL/TURNO/INDIVIDUO_TURNO/FORECAST_7072_INDIVIDUO_TURNO.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ads operator run -f forecast/COPEL/TURNO/INDIVIDUO_TURNO/FORECAST_1510_INDIVIDUO_TURNO.yaml -b local',\n",
       " 'ads operator run -f forecast/COPEL/TURNO/INDIVIDUO_TURNO/FORECAST_5428_INDIVIDUO_TURNO.yaml -b local',\n",
       " 'ads operator run -f forecast/COPEL/TURNO/INDIVIDUO_TURNO/FORECAST_7072_INDIVIDUO_TURNO.yaml -b local']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criação dos YAMLs para turno\n",
    "create_forecast_yaml(\n",
    "    datasets= datasets_turno,\n",
    "    base_folder= base_folder_turno,\n",
    "    output_folder= output_folder_turno,\n",
    "    timestamp_col= timestamp_col,\n",
    "    target_col= target_col_turno,\n",
    "    horizon= horizon_turno,\n",
    "    model= model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d888c5d-9ab9-4343-863a-94304278239d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo YAML criado: forecast/COPEL/HORA/TEMPO_TOTAL_SIMPLES/FORECAST_1510_TEMPO_TOTAL_SIMPLES.yaml\n",
      "Arquivo YAML criado: forecast/COPEL/HORA/TEMPO_TOTAL_SIMPLES/FORECAST_5428_TEMPO_TOTAL_SIMPLES.yaml\n",
      "Arquivo YAML criado: forecast/COPEL/HORA/TEMPO_TOTAL_SIMPLES/FORECAST_7072_TEMPO_TOTAL_SIMPLES.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ads operator run -f forecast/COPEL/HORA/TEMPO_TOTAL_SIMPLES/FORECAST_1510_TEMPO_TOTAL_SIMPLES.yaml -b local',\n",
       " 'ads operator run -f forecast/COPEL/HORA/TEMPO_TOTAL_SIMPLES/FORECAST_5428_TEMPO_TOTAL_SIMPLES.yaml -b local',\n",
       " 'ads operator run -f forecast/COPEL/HORA/TEMPO_TOTAL_SIMPLES/FORECAST_7072_TEMPO_TOTAL_SIMPLES.yaml -b local']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criação dos YAMLs para hora\n",
    "create_forecast_yaml(\n",
    "    datasets= datasets_hora,\n",
    "    base_folder= base_folder_hora,\n",
    "    output_folder= output_folder_hora,\n",
    "    timestamp_col= timestamp_col,\n",
    "    target_col= target_col_hora,\n",
    "    horizon= horizon_hora,\n",
    "    model= model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "250dfff8-7e18-48ff-8bac-4f24c0459ec0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo combinado salvo em: COPEL_OUTPUT/combined_forecast_INDIVIDUO_TURNO.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lista de arquivos e respectivos códigos\n",
    "files_and_codes = [\n",
    "    (f\"COPEL_OUTPUT/1510/{target_col_turno}/forecast.csv\", 1510),\n",
    "    (f\"COPEL_OUTPUT/5428/{target_col_turno}/forecast.csv\", 5428),\n",
    "    (f\"COPEL_OUTPUT/7072/{target_col_turno}/forecast.csv\", 7072)\n",
    "]\n",
    "\n",
    "# Lista para armazenar os DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterar pelos arquivos e adicionar a coluna COD_LOCAL\n",
    "for file_path, code in files_and_codes:\n",
    "    # Lê o CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Adiciona a coluna COD_LOCAL\n",
    "    df['COD_LOCAL'] = code\n",
    "    \n",
    "    # Adiciona o DataFrame à lista\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Une todos os DataFrames em um único DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Salva o DataFrame combinado em um novo arquivo CSV\n",
    "output_path = f\"COPEL_OUTPUT/combined_forecast_{target_col_turno}.csv\"\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Arquivo combinado salvo em: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d920701-e91b-4256-8454-3a490e79a6f5",
   "metadata": {},
   "source": [
    "## **[FORECAST]** União das tabelas 1510 | 5428 | 7072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52c3bead-ebc8-4f25-a38f-443a301cf0af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo combinado salvo em: COPEL_OUTPUT/combined_forecast_TEMPO_TOTAL_SIMPLES.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lista de arquivos e respectivos códigos\n",
    "files_and_codes = [\n",
    "    (f\"COPEL_OUTPUT/1510/{target_col_hora}/forecast.csv\", 1510),\n",
    "    (f\"COPEL_OUTPUT/5428/{target_col_hora}/forecast.csv\", 5428),\n",
    "    (f\"COPEL_OUTPUT/7072/{target_col_hora}/forecast.csv\", 7072)\n",
    "]\n",
    "\n",
    "# Lista para armazenar os DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterar pelos arquivos e adicionar a coluna COD_LOCAL\n",
    "for file_path, code in files_and_codes:\n",
    "    # Lê o CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Adiciona a coluna COD_LOCAL\n",
    "    df['COD_LOCAL'] = code\n",
    "    \n",
    "    # Adiciona o DataFrame à lista\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Une todos os DataFrames em um único DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Salva o DataFrame combinado em um novo arquivo CSV\n",
    "output_path = f\"COPEL_OUTPUT/combined_forecast_{target_col_hora}.csv\"\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Arquivo combinado salvo em: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39af2ae-62ef-4bef-8bcd-455dc7ff6179",
   "metadata": {},
   "source": [
    "## **[FORECAST]** Encaminhamento das tabelas ao Banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3fb2516-c122-410a-ad7d-a6b28380df9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn = oracledb.connect(user=\"FORECAST\", password=\"Oracle123#analytics\", dsn=\"(description= (retry_count=20)(retry_delay=3)(address=(protocol=tcps)(port=1522)(host=adb.us-ashburn-1.oraclecloud.com))(connect_data=(service_name=arj21cwbhl1zqnf_opendata_medium.adwc.oraclecloud.com))(security=(ssl_server_dn_match=yes)))\",\n",
    "                              config_dir=\"/home/datascience/[WALLET] 23AI\",\n",
    "                              wallet_location=\"/home/datascience/[WALLET] 23AI\",\n",
    "                              wallet_password=\"Oracle123#analytics\")\n",
    "    print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(\"Connection failed!\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9998b313-d1fb-4ca3-9d8d-d5263b0874c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela OUTPUT_TEMPO_TOTAL_SIMPLES criada com sucesso!\n",
      "Conexão encerrada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Criar tabela usando o estilo direto\n",
    "cur = conn.cursor()\n",
    "try:\n",
    "    # Query para criar a tabela\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE OUTPUT_{target_col_hora} (\n",
    "        ID NUMBER GENERATED ALWAYS AS IDENTITY,\n",
    "        DATA_REGISTRO VARCHAR2(100),\n",
    "        SERIES VARCHAR2(100),\n",
    "        INPUT_VALUE NUMBER,\n",
    "        FITTED_VALUE NUMBER,\n",
    "        FORECAST_VALUE NUMBER,\n",
    "        P10 NUMBER,\n",
    "        P90 NUMBER,\n",
    "        COD_LOCAL VARCHAR2(100),\n",
    "        PRIMARY KEY (ID)\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Executa a query\n",
    "    cur.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print(f\"Tabela OUTPUT_{target_col_hora} criada com sucesso!\")\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print(\"Conexão encerrada com sucesso!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "013541b2-c609-4110-8cba-c170456d15b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos os registros da tabela OUTPUT_TEMPO_TOTAL_SIMPLES foram excluídos.\n",
      "5418 registros substituídos com sucesso na tabela OUTPUT_TEMPO_TOTAL_SIMPLES.\n",
      "Conexão encerrada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Renomeia as colunas para corresponder à tabela do banco\n",
    "combined_df = combined_df.rename(columns={\n",
    "    \"Date\": \"DATA_REGISTRO\",\n",
    "    \"Series\": \"SERIES\",\n",
    "    \"input_value\": \"INPUT_VALUE\",\n",
    "    \"fitted_value\": \"FITTED_VALUE\",\n",
    "    \"forecast_value\": \"FORECAST_VALUE\",\n",
    "    \"p10\": \"P10\",\n",
    "    \"p90\": \"P90\",\n",
    "    \"COD_LOCAL\": \"COD_LOCAL\"\n",
    "})\n",
    "\n",
    "\n",
    "# Ajuste das colunas do DataFrame\n",
    "combined_df[\"DATA_REGISTRO\"] = combined_df[\"DATA_REGISTRO\"].astype(str)  # Converte para string\n",
    "combined_df[\"FORECAST_VALUE\"] = combined_df[\"FORECAST_VALUE\"].fillna(value=0).astype(float)\n",
    "combined_df[\"P10\"] = combined_df[\"P10\"].fillna(value=0).astype(float)\n",
    "combined_df[\"P90\"] = combined_df[\"P90\"].fillna(value=0).astype(float)\n",
    "combined_df[\"INPUT_VALUE\"] = combined_df[\"INPUT_VALUE\"].fillna(value=0).astype(float)\n",
    "combined_df[\"FITTED_VALUE\"] = combined_df[\"FITTED_VALUE\"].fillna(value=0).astype(float)\n",
    "combined_df[\"COD_LOCAL\"] = combined_df[\"COD_LOCAL\"].astype(str)\n",
    "\n",
    "# combined_df[\"FORECAST_VALUE\"] = combined_df[\"FORECAST_VALUE\"].where(combined_df[\"FORECAST_VALUE\"].notna(), None)\n",
    "# combined_df[\"P10\"] = combined_df[\"P10\"].where(combined_df[\"P10\"].notna(), None)\n",
    "# combined_df[\"P90\"] = combined_df[\"P90\"].where(combined_df[\"P90\"].notna(), None)\n",
    "# combined_df[\"INPUT_VALUE\"] = combined_df[\"INPUT_VALUE\"].where(combined_df[\"INPUT_VALUE\"].notna(), None)\n",
    "# combined_df[\"FITTED_VALUE\"] = combined_df[\"FITTED_VALUE\"].where(combined_df[\"FITTED_VALUE\"].notna(), None)\n",
    "\n",
    "\n",
    "# Reordena as colunas para corresponder à tabela no banco\n",
    "combined_df = combined_df[[\n",
    "    \"DATA_REGISTRO\", \"SERIES\", \"INPUT_VALUE\", \"FITTED_VALUE\", \"FORECAST_VALUE\", \"P10\", \"P90\", \"COD_LOCAL\"\n",
    "]]\n",
    "\n",
    "# Criar o cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Deletar todos os dados existentes na tabela antes de inserir os novos\n",
    "delete_query = f\"DELETE FROM OUTPUT_{target_col_hora}\"\n",
    "cursor.execute(delete_query)\n",
    "print(f\"Todos os registros da tabela OUTPUT_{target_col_hora} foram excluídos.\")\n",
    "\n",
    "# Query para inserção\n",
    "insert_query = f\"\"\"\n",
    "    INSERT INTO OUTPUT_{target_col_hora} (\n",
    "        DATA_REGISTRO, SERIES, INPUT_VALUE, FITTED_VALUE, FORECAST_VALUE, P10, P90, COD_LOCAL\n",
    "    ) VALUES (:1, :2, :3, :4, :5, :6, :7, :8)\n",
    "    \"\"\"\n",
    "\n",
    "# Converter o DataFrame em uma lista de tuplas\n",
    "data_to_insert = combined_df.to_records(index=False)\n",
    "data_to_insert = [tuple(row) for row in data_to_insert]\n",
    "\n",
    "# Inserir os dados\n",
    "cursor.executemany(insert_query, data_to_insert)\n",
    "print(f\"{cursor.rowcount} registros substituídos com sucesso na tabela OUTPUT_{target_col_hora}.\")\n",
    "\n",
    "# Commit da transação\n",
    "conn.commit()\n",
    "\n",
    "# Fechar o cursor e a conexão\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Conexão encerrada com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "15f3759c-f5fb-4e57-8c62-a240bf7316f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4518 registros inseridos com sucesso na tabela OUTPUT_TEMPO_TOTAL_SIMPLES.\n",
      "Conexão encerrada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Renomeia as colunas para corresponder à tabela do banco\n",
    "combined_df = combined_df.rename(columns={\n",
    "    \"Date\": \"DATA_REGISTRO\",\n",
    "    \"Series\": \"SERIES\",\n",
    "    \"input_value\": \"INPUT_VALUE\",\n",
    "    \"fitted_value\": \"FITTED_VALUE\",\n",
    "    \"forecast_value\": \"FORECAST_VALUE\",\n",
    "    \"p10\": \"P10\",\n",
    "    \"p90\": \"P90\",\n",
    "    \"COD_LOCAL\": \"COD_LOCAL\"\n",
    "})\n",
    "\n",
    "\n",
    "# Ajuste das colunas do DataFrame\n",
    "combined_df[\"DATA_REGISTRO\"] = combined_df[\"DATA_REGISTRO\"].astype(str)  # Converte para string\n",
    "combined_df[\"FORECAST_VALUE\"] = combined_df[\"FORECAST_VALUE\"].fillna(value=0).astype(float)\n",
    "combined_df[\"P10\"] = combined_df[\"P10\"].fillna(value=0).astype(float)\n",
    "combined_df[\"P90\"] = combined_df[\"P90\"].fillna(value=0).astype(float)\n",
    "combined_df[\"INPUT_VALUE\"] = combined_df[\"INPUT_VALUE\"].fillna(value=0).astype(float)\n",
    "combined_df[\"FITTED_VALUE\"] = combined_df[\"FITTED_VALUE\"].fillna(value=0).astype(float)\n",
    "combined_df[\"COD_LOCAL\"] = combined_df[\"COD_LOCAL\"].astype(str)\n",
    "\n",
    "# combined_df[\"FORECAST_VALUE\"] = combined_df[\"FORECAST_VALUE\"].where(combined_df[\"FORECAST_VALUE\"].notna(), None)\n",
    "# combined_df[\"P10\"] = combined_df[\"P10\"].where(combined_df[\"P10\"].notna(), None)\n",
    "# combined_df[\"P90\"] = combined_df[\"P90\"].where(combined_df[\"P90\"].notna(), None)\n",
    "# combined_df[\"INPUT_VALUE\"] = combined_df[\"INPUT_VALUE\"].where(combined_df[\"INPUT_VALUE\"].notna(), None)\n",
    "# combined_df[\"FITTED_VALUE\"] = combined_df[\"FITTED_VALUE\"].where(combined_df[\"FITTED_VALUE\"].notna(), None)\n",
    "\n",
    "\n",
    "# Reordena as colunas para corresponder à tabela no banco\n",
    "combined_df = combined_df[[\n",
    "    \"DATA_REGISTRO\", \"SERIES\", \"INPUT_VALUE\", \"FITTED_VALUE\", \"FORECAST_VALUE\", \"P10\", \"P90\", \"COD_LOCAL\"\n",
    "]]\n",
    "\n",
    "    # Criar o cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "    # Query para inserção\n",
    "insert_query = f\"\"\"\n",
    "    INSERT INTO OUTPUT_{target_col_hora} (\n",
    "        DATA_REGISTRO, SERIES, INPUT_VALUE, FITTED_VALUE, FORECAST_VALUE, P10, P90, COD_LOCAL\n",
    "    ) VALUES (:1, :2, :3, :4, :5, :6, :7, :8)\n",
    "    \"\"\"\n",
    "\n",
    "    # Converter o DataFrame em uma lista de tuplas\n",
    "data_to_insert = combined_df.to_records(index=False)\n",
    "data_to_insert = [tuple(row) for row in data_to_insert]\n",
    "\n",
    "    # Inserir os dados\n",
    "cursor.executemany(insert_query, data_to_insert)\n",
    "print(f\"{cursor.rowcount} registros inseridos com sucesso na tabela OUTPUT_{target_col_hora}.\")\n",
    "\n",
    "    # Commit da transação\n",
    "conn.commit()\n",
    "\n",
    "    # Fechar o cursor e a conexão\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Conexão encerrada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6be8f-5cd0-420d-9e3e-e19ab04af177",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TURNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e5e41fd8-62e7-490a-8d52-817312b68ed8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela OUTPUT_INDIVIDUO_TURNO criada com sucesso!\n",
      "Conexão encerrada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Criar tabela usando o estilo direto\n",
    "cur = conn.cursor()\n",
    "try:\n",
    "    # Query para criar a tabela\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE OUTPUT_{target_col_hora} (\n",
    "        ID NUMBER GENERATED ALWAYS AS IDENTITY,\n",
    "        DATA_REGISTRO VARCHAR2(100),\n",
    "        SERIES VARCHAR2(100),\n",
    "        INPUT_VALUE NUMBER,\n",
    "        FITTED_VALUE NUMBER,\n",
    "        FORECAST_VALUE NUMBER,\n",
    "        P10 NUMBER,\n",
    "        P90 NUMBER,\n",
    "        COD_LOCAL VARCHAR2(100),\n",
    "        PRIMARY KEY (ID)\n",
    "    )\n",
    "    \"\"\"\n",
    "    # Executa a query\n",
    "    cur.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    print(f\"Tabela OUTPUT_{target_col_hora} criada com sucesso!\")\n",
    "finally:\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print(\"Conexão encerrada com sucesso!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "29b37af3-5658-4ee7-b9fb-0efe5df473bb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875 registros inseridos com sucesso na tabela OUTPUT_INDIVIDUO_TURNO.\n",
      "Conexão encerrada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Renomeia as colunas para corresponder à tabela do banco\n",
    "combined_df = combined_df.rename(columns={\n",
    "    \"Date\": \"DATA_REGISTRO\",\n",
    "    \"Series\": \"SERIES\",\n",
    "    \"input_value\": \"INPUT_VALUE\",\n",
    "    \"fitted_value\": \"FITTED_VALUE\",\n",
    "    \"forecast_value\": \"FORECAST_VALUE\",\n",
    "    \"p10\": \"P10\",\n",
    "    \"p90\": \"P90\",\n",
    "    \"COD_LOCAL\": \"COD_LOCAL\"\n",
    "})\n",
    "\n",
    "\n",
    "# Ajuste das colunas do DataFrame\n",
    "combined_df[\"DATA_REGISTRO\"] = combined_df[\"DATA_REGISTRO\"].astype(str)  # Converte para string\n",
    "combined_df[\"FORECAST_VALUE\"] = combined_df[\"FORECAST_VALUE\"].where(combined_df[\"FORECAST_VALUE\"].notna(), None)\n",
    "combined_df[\"P10\"] = combined_df[\"P10\"].where(combined_df[\"P10\"].notna(), None)\n",
    "combined_df[\"P90\"] = combined_df[\"P90\"].where(combined_df[\"P90\"].notna(), None)\n",
    "combined_df[\"INPUT_VALUE\"] = combined_df[\"INPUT_VALUE\"].where(combined_df[\"INPUT_VALUE\"].notna(), None)\n",
    "combined_df[\"FITTED_VALUE\"] = combined_df[\"FITTED_VALUE\"].where(combined_df[\"FITTED_VALUE\"].notna(), None)\n",
    "\n",
    "\n",
    "# combined_df[\"FORECAST_VALUE\"] = combined_df[\"FORECAST_VALUE\"].fillna(value=0).astype(float)\n",
    "# combined_df[\"P10\"] = combined_df[\"P10\"].fillna(value=0).astype(float)\n",
    "# combined_df[\"P90\"] = combined_df[\"P90\"].fillna(value=0).astype(float)\n",
    "# combined_df[\"INPUT_VALUE\"] = combined_df[\"INPUT_VALUE\"].fillna(value=0).astype(float)\n",
    "# combined_df[\"FITTED_VALUE\"] = combined_df[\"FITTED_VALUE\"].fillna(value=0).astype(float)\n",
    "\n",
    "\n",
    "# Reordena as colunas para corresponder à tabela no banco\n",
    "combined_df = combined_df[[\n",
    "    \"DATA_REGISTRO\", \"SERIES\", \"INPUT_VALUE\", \"FITTED_VALUE\", \"FORECAST_VALUE\", \"P10\", \"P90\", \"COD_LOCAL\"\n",
    "]]\n",
    "\n",
    "    # Criar o cursor\n",
    "cursor = conn.cursor()\n",
    "\n",
    "    # Query para inserção\n",
    "insert_query = f\"\"\"\n",
    "    INSERT INTO OUTPUT_{target_col_turno} (\n",
    "        DATA_REGISTRO, SERIES, INPUT_VALUE, FITTED_VALUE, FORECAST_VALUE, P10, P90, COD_LOCAL\n",
    "    ) VALUES (:1, :2, :3, :4, :5, :6, :7, :8)\n",
    "    \"\"\"\n",
    "\n",
    "    # Converter o DataFrame em uma lista de tuplas\n",
    "data_to_insert = combined_df.to_records(index=False)\n",
    "data_to_insert = [tuple(row) for row in data_to_insert]\n",
    "\n",
    "    # Inserir os dados\n",
    "cursor.executemany(insert_query, data_to_insert)\n",
    "print(f\"{cursor.rowcount} registros inseridos com sucesso na tabela OUTPUT_{target_col_turno}.\")\n",
    "\n",
    "    # Commit da transação\n",
    "conn.commit()\n",
    "\n",
    "    # Fechar o cursor e a conexão\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Conexão encerrada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e46233-b32f-490f-95a0-12c206976e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def insert_data_into_table(connection, dataframe, table_name):\n",
    "    \"\"\"\n",
    "    Insere os dados de um DataFrame na tabela especificada no banco Oracle.\n",
    "\n",
    "    Parameters:\n",
    "        connection (oracledb.Connection): Conexão ativa com o banco.\n",
    "        dataframe (pd.DataFrame): DataFrame com os dados a serem inseridos.\n",
    "        table_name (str): Nome da tabela no banco.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Prepara a query de inserção\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {table_name} (\n",
    "            DATE, SERIES, INPUT_VALUE, FITTED_VALUE, FORECAST_VALUE, P10, P90, COD_LOCAL\n",
    "        ) VALUES (:1, :2, :3, :4, :5, :6, :7, :8)\n",
    "        \"\"\"\n",
    "\n",
    "        # Converte o DataFrame para uma lista de tuplas\n",
    "        data_to_insert = dataframe.to_records(index=False)\n",
    "        data_to_insert = [tuple(row) for row in data_to_insert]\n",
    "\n",
    "        # Insere os dados\n",
    "        cursor.executemany(insert_query, data_to_insert)\n",
    "        print(f\"{cursor.rowcount} registros inseridos com sucesso na tabela {table_name}.\")\n",
    "\n",
    "        # Confirma a transação\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "    except oracledb.Error as e:\n",
    "        print(\"Erro ao inserir os dados na tabela:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515223d-9473-469e-bd4b-011f2adb82ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df[\"DATE\"] = pd.to_datetime(combined_df[\"DATE\"])\n",
    "combined_df[\"FORECAST_VALUE\"] = combined_df[\"FORECAST_VALUE\"].astype(float)\n",
    "combined_df[\"P10\"] = combined_df[\"P10\"].astype(float)\n",
    "combined_df[\"P90\"] = combined_df[\"P90\"].astype(float)\n",
    "\n",
    "# Inserir os dados na tabela\n",
    "insert_data_into_table(conn, combined_df, \"OUTPUT_FORECAST\")\n",
    "\n",
    "# Fechar a conexão\n",
    "conn.close()\n",
    "print(\"Conexão encerrada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19b8bca2-2c36-448a-af34-5563c2948bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Remove timestamps duplicados\n",
    "# results_turno['turno_1510']['test_data'] = results_turno['turno_1510']['test_data'].drop_duplicates(subset='HORARIO_COMPLETO')\n",
    "# results_turno['turno_1510']['train_data'] = results_turno['turno_1510']['train_data'].drop_duplicates(subset='HORARIO_COMPLETO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab3b28a8-4517-421d-89d8-613a0f47d40e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervalos inconsistentes:\n",
      "396               NaT\n",
      "848   0 days 06:00:00\n",
      "530   0 days 08:00:00\n",
      "502   0 days 06:00:00\n",
      "651   0 days 08:00:00\n",
      "            ...      \n",
      "573   0 days 08:00:00\n",
      "575   0 days 14:00:00\n",
      "706   0 days 06:00:00\n",
      "258   0 days 08:00:00\n",
      "169   0 days 14:00:00\n",
      "Name: HORARIO_COMPLETO, Length: 63, dtype: timedelta64[ns]\n"
     ]
    }
   ],
   "source": [
    "# # Calcula as diferenças entre os timestamps\n",
    "# time_diffs = results_turno['turno_1510']['test_data']['HORARIO_COMPLETO'].diff()\n",
    "\n",
    "# # Exibe intervalos que não correspondem à frequência esperada\n",
    "# print(\"Intervalos inconsistentes:\")\n",
    "# print(time_diffs[time_diffs != pd.Timedelta('10H')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8463cf9b-1548-4fb7-bc80-7ab8fcfdef96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results_turno['turno_1510']['test_data'].to_csv(\"COPEL_INPUT/INDIVIDUO_TURNO/COPEL_1510/COPEL_TEST.csv\", index=False)\n",
    "# results_turno['turno_1510']['train_data'].to_csv(\"COPEL_INPUT/INDIVIDUO_TURNO/COPEL_1510/COPEL_TRAIN.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "114246b3-f207-47f4-8a7b-4b056f00cbe8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HORARIO_COMPLETO    datetime64[ns]\n",
       "INDIVIDUO_TURNO            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_turno['turno_1510']['test_data'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd94903d-eb19-4fa3-a79d-49be667235b1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexão bem-sucedida com o banco Oracle (modo Thin)!\n",
      "Teste de consulta bem-sucedido! Resultado: (1,)\n",
      "Conexão encerrada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import oracledb\n",
    "\n",
    "# Configurações para o modo Thin\n",
    "user = \"FORECAST\"\n",
    "password = \"Oracle123#analytics\"\n",
    "dsn = \"\"\"\n",
    "(description=\n",
    "    (retry_count=20)\n",
    "    (retry_delay=3)\n",
    "    (address=(protocol=tcps)(port=1522)(host=adb.us-ashburn-1.oraclecloud.com))\n",
    "    (connect_data=(service_name=arj21cwbhl1zqnf_opendata_medium.adwc.oraclecloud.com))\n",
    "    (security=(ssl_server_dn_match=yes))\n",
    ")\n",
    "\"\"\"\n",
    "wallet_location = \"/home/datascience/[WALLET] 23AI\"\n",
    "wallet_password = \"Oracle123#analytics\"\n",
    "\n",
    "try:\n",
    "    # Conectando ao banco no modo Thin\n",
    "    connection = oracledb.connect(\n",
    "        user=user,\n",
    "        password=password,\n",
    "        dsn=dsn,\n",
    "        wallet_location=wallet_location,\n",
    "        wallet_password=wallet_password\n",
    "    )\n",
    "    print(\"Conexão bem-sucedida com o banco Oracle (modo Thin)!\")\n",
    "\n",
    "    # Consulta simples para testar\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT 1 FROM DUAL\")\n",
    "    result = cursor.fetchone()\n",
    "    print(\"Teste de consulta bem-sucedido! Resultado:\", result)\n",
    "\n",
    "    # Encerrando o cursor e a conexão\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(\"Conexão encerrada com sucesso!\")\n",
    "except oracledb.Error as e:\n",
    "    print(\"Erro ao conectar ao banco Oracle:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:forecast_p310_cpu_x86_64_v4]",
   "language": "python",
   "name": "conda-env-forecast_p310_cpu_x86_64_v4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
